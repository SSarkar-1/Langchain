{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9986cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6cd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import asyncio\n",
    "\n",
    "#Fix Windoiws issues in jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(),asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirectstderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr=sys.__stderr__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc869a6e",
   "metadata": {},
   "source": [
    "# Local MCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d78eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client=MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\":{\n",
    "            \"transport\":\"stdio\",\n",
    "            \"command\":\"python\",\n",
    "            \"args\":[\"C:/Users/SHUBHAM SARKAR/Desktop/Agentic/lca-lc-foundations/notebooks/module-2/resources/2.1_mcp_server.py\"],\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d9cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47501a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "agent=create_agent(model=\"gpt-5-nano\",tools=tools,system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4a4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config={\"configurable\":{\"thread_id\":\"1\"}}\n",
    "\n",
    "response= await agent.ainvoke(\n",
    "    {\"messages\":[HumanMessage(content=\"tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50157a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='6cc19092-a6e0-42b2-99a5-adfcb6010b39'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 284, 'prompt_tokens': 271, 'total_tokens': 555, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CrfbeRVU8ycA3IJ0mWbCmF4z6XytY', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b63e2-96fc-7eb0-bdbc-22ac9f35fcb8-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_DbuiPahxxb5hRgC1ap0Y6doR', 'type': 'tool_call'}], usage_metadata={'input_tokens': 271, 'output_tokens': 284, 'total_tokens': 555, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999956,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"# `langchain-mcp-adapters`Â¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\\\\\">\\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\\\\\">session | Connect to an MCP server and initialize a session. | get\\\\\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\\\\\">get\\\\\\\\_tools | Get a list of all tools from all connected servers. | get\\\\\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\\\\\">get\\\\\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_tools `async` Â¶. (langchain_mcp_adapters.callbacks.Callbacks)\\\\\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\\\\\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\\\\\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_resources `async` Â¶. | \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\\\\\">\\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ | Intercept tool execution with control over handler invocation.\",\\n      \"score\": 0.9999784,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters\",\\n      \"title\": \"LangChain MCP Integration: Complete Guide to ...\",\\n      \"content\": \"from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\\\\\"python\\\\\", \\\\\"database_mcp_server.py\\\\\"], transport_type=\\\\\"stdio\\\\\", environment={ \\\\\"DATABASE_URL\\\\\": \\\\\"postgresql://user:pass@localhost:5432/mydb\\\\\", \\\\\"MAX_CONNECTIONS\\\\\": \\\\\"10\\\\\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\\\\\"gpt-4\\\\\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Find all customers who made purchases over $500 in the last month\\\\\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\\\\\"http://localhost:3000/mcp\\\\\", transport_type=\\\\\"sse\\\\\", headers={ \\\\\"Authorization\\\\\": f\\\\\"Bearer {os.getenv(\\'API_TOKEN\\')}\\\\\", \\\\\"User-Agent\\\\\": \\\\\"LangChain-MCP-Client/1.0\\\\\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Create a new lead for John Smith with email [email\\xa0protected]\\\\\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.\",\\n      \"score\": 0.9999621,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain ðŸ”Œ MCP\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.9999461,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://composio.dev/blog/langchain-mcp-adapter-a-step-by-step-guide-to-build-mcp-agents\",\\n      \"title\": \"LangChain MCP Adapter: A step-by-step guide to build ...\",\\n      \"content\": \"import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{createReactAgent} from\\\\\"@langchain/langgraph/prebuilt\\\\\"; import{HumanMessage, AIMessage} from\\\\\"@langchain/core/messages\\\\\"; import dotenv from \\\\\"dotenv\\\\\"; import* as readline from\\\\\"node:readline/promises\\\\\"; import{stdin as input, stdout as output} from\\\\\"node:process\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\"; dotenv config();// ðŸ§© Add any MCP tool URLs youâ€™ve set up with Composio const mcpServers{// Example:// gmail: {// transport: \\\\\"sse\\\\\",// url: \\\\\"\\\\\"// }}; async function runChat(){const rl readline createInterface({input, output}); const chatHistory[]; const client new MultiServerMCPClient(mcpServers); const tools await client getTools(); const model new ChatOpenAI({modelName:\\\\\"gpt-4o\\\\\", temperature: 0, openAIApiKey: process env OPENAI_API_KEY}); const agent createReactAgent({llm: model, tools}); console log(\\\\\"Agent is ready. import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{createReactAgent} from\\\\\"@langchain/langgraph/prebuilt\\\\\"; import{HumanMessage, AIMessage} from\\\\\"@langchain/core/messages\\\\\"; import dotenv from \\\\\"dotenv\\\\\"; import* as readline from\\\\\"node:readline/promises\\\\\"; import{stdin as input, stdout as output} from\\\\\"node:process\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\"; dotenv config();// ðŸ§© Add any MCP tool URLs youâ€™ve set up with Composio const mcpServers{// Example:// gmail: {// transport: \\\\\"sse\\\\\",// url: \\\\\"\\\\\"// }}; async function runChat(){const rl readline createInterface({input, output}); const chatHistory[]; const client new MultiServerMCPClient(mcpServers); const tools await client getTools(); const model new ChatOpenAI({modelName:\\\\\"gpt-4o\\\\\", temperature: 0, openAIApiKey: process env OPENAI_API_KEY}); const agent createReactAgent({llm: model, tools}); console log(\\\\\"Agent is ready.\",\\n      \"score\": 0.9999292,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.0,\\n  \"request_id\": \"d74a08a7-8370-42f5-add3-459aeab64200\"\\n}', 'id': 'lc_0d5e1163-8151-4b30-8953-7b3aa627946d'}], name='search_web', id='5344c7ce-fffc-4789-916d-169cec500dc1', tool_call_id='call_DbuiPahxxb5hRgC1ap0Y6doR', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9999956, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': '# `langchain-mcp-adapters`Â¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\_\\\\_init\\\\_\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\">\\\\_\\\\_init\\\\_\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\">session | Connect to an MCP server and initialize a session. | get\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\">get\\\\_tools | Get a list of all tools from all connected servers. | get\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\">get\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_tools `async` Â¶. (langchain_mcp_adapters.callbacks.Callbacks)\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_resources `async` Â¶. | \\\\_\\\\_call\\\\_\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\">\\\\_\\\\_call\\\\_\\\\_ | Intercept tool execution with control over handler invocation.', 'score': 0.9999784, 'raw_content': None}, {'url': 'https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters', 'title': 'LangChain MCP Integration: Complete Guide to ...', 'content': 'from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\"python\", \"database_mcp_server.py\"], transport_type=\"stdio\", environment={ \"DATABASE_URL\": \"postgresql://user:pass@localhost:5432/mydb\", \"MAX_CONNECTIONS\": \"10\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\"gpt-4\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \"input\": \"Find all customers who made purchases over $500 in the last month\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\"http://localhost:3000/mcp\", transport_type=\"sse\", headers={ \"Authorization\": f\"Bearer {os.getenv(\\'API_TOKEN\\')}\", \"User-Agent\": \"LangChain-MCP-Client/1.0\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \"input\": \"Create a new lead for John Smith with email [email\\xa0protected]\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.', 'score': 0.9999621, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters: LangChain ðŸ”Œ MCP', 'content': 'from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"/path/to/math_server.py\" \"transport\" \"stdio\" \"weather\" # Make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools agent = create_agent\"openai:gpt-4.1\" tools math_response = await agent ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await agent ainvoke \"messages\" \"what is the weather in nyc?\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\"openai:gpt-4.1\" client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"./examples/math_server.py\" \"transport\" \"stdio\" \"weather\" # make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \"messages\" return \"messages\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \"call_model\" builder add_conditional_edges \"call_model\" tools_condition builder add_edge \"tools\" \"call_model\" graph = builder compile math_response = await graph ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await graph ainvoke \"messages\" \"what is the weather in nyc?\".', 'score': 0.9999461, 'raw_content': None}, {'url': 'https://composio.dev/blog/langchain-mcp-adapter-a-step-by-step-guide-to-build-mcp-agents', 'title': 'LangChain MCP Adapter: A step-by-step guide to build ...', 'content': 'import{ChatOpenAI} from\"@langchain/openai\"; import{createReactAgent} from\"@langchain/langgraph/prebuilt\"; import{HumanMessage, AIMessage} from\"@langchain/core/messages\"; import dotenv from \"dotenv\"; import* as readline from\"node:readline/promises\"; import{stdin as input, stdout as output} from\"node:process\"; import{MultiServerMCPClient} from\"@langchain/mcp-adapters\"; dotenv config();// ðŸ§© Add any MCP tool URLs youâ€™ve set up with Composio const mcpServers{// Example:// gmail: {// transport: \"sse\",// url: \"\"// }}; async function runChat(){const rl readline createInterface({input, output}); const chatHistory[]; const client new MultiServerMCPClient(mcpServers); const tools await client getTools(); const model new ChatOpenAI({modelName:\"gpt-4o\", temperature: 0, openAIApiKey: process env OPENAI_API_KEY}); const agent createReactAgent({llm: model, tools}); console log(\"Agent is ready. import{ChatOpenAI} from\"@langchain/openai\"; import{createReactAgent} from\"@langchain/langgraph/prebuilt\"; import{HumanMessage, AIMessage} from\"@langchain/core/messages\"; import dotenv from \"dotenv\"; import* as readline from\"node:readline/promises\"; import{stdin as input, stdout as output} from\"node:process\"; import{MultiServerMCPClient} from\"@langchain/mcp-adapters\"; dotenv config();// ðŸ§© Add any MCP tool URLs youâ€™ve set up with Composio const mcpServers{// Example:// gmail: {// transport: \"sse\",// url: \"\"// }}; async function runChat(){const rl readline createInterface({input, output}); const chatHistory[]; const client new MultiServerMCPClient(mcpServers); const tools await client getTools(); const model new ChatOpenAI({modelName:\"gpt-4o\", temperature: 0, openAIApiKey: process env OPENAI_API_KEY}); const agent createReactAgent({llm: model, tools}); console log(\"Agent is ready.', 'score': 0.9999292, 'raw_content': None}], 'response_time': 0.0, 'request_id': 'd74a08a7-8370-42f5-add3-459aeab64200'}}}),\n",
      "              AIMessage(content='Hereâ€™s a concise overview of the LangChain MCP Adapters library and what itâ€™s for.\\n\\nWhat it is\\n- A bridge between Anthropicâ€™s Model Context Protocol (MCP) tools and LangChain/LangGraph.\\n- Lets you load MCP tools from one or more MCP servers and use them as LangChain tools, prompts, and resources.\\n- Enables agents (including LangGraph-based agents) to interact with tools across multiple MCP servers in a seamless way.\\n\\nKey features\\n- Converts MCP tools into LangChain- and LangGraph-compatible tools automatically.\\n- Supports multi-server setups, so an agent can pull tools from several MCP servers at once.\\n- Integrates MCP tool servers into LangGraph agents, enabling richer tool ecosystems without custom adapters for each server.\\n- Provides a client (MultiServerMCPClient) to manage connections, discovery, and loading of tools/prompts/resources.\\n\\nCore components\\n- MultiServerMCPClient\\n  - Manages connections to multiple MCP servers.\\n  - Loads LangChain-compatible tools, prompts, and resources from MCP servers.\\n  - Async methods including:\\n    - session: establish a session with the MCP servers\\n    - get_tools: fetch a list of all tools from all connected servers\\n    - get_resources: fetch resources from MCP servers\\n  - load_mcp_tools (async): load tools from MCP servers into LangChain, with options for callbacks, tool interceptors, server naming, and tool name prefixes\\n  - load_mcp_resources (async): load MCP resources (prompts, etc.)\\n- Integration points\\n  - Works with LangChain agents (e.g., create_agent or similar) to build tool-enabled workflows.\\n  - Can be used with LangGraph workflows and prebuilt graphs that include ToolNodes and state graphs.\\n\\nUsage flow (high level)\\n1) Install the package (along with LangChain dependencies).\\n2) Define your MCP servers (names, transport method, URLs/paths, etc.).\\n3) Instantiate MultiServerMCPClient with those servers.\\n4) Open a session and load tools (client.get_tools()).\\n5) Create a LangChain/LangGraph agent using the loaded tools.\\n6) Run queries and have the agent invoke tools across the MCP servers.\\n\\nWhy youâ€™d use it\\n- To quickly connect LangChain/LangGraph to a large ecosystem of MCP tool servers without writing a lot of per-tool adapters.\\n- To enable agents to pull tools from multiple MCP servers simultaneously, enabling richer capabilities and workflows.\\n\\nWhere to read more (official docs and context)\\n- LangChain changelog post: MCP Adapters for LangChain and LangGraph â€” explains the purpose and benefits of the adapters.\\n- LangChain docs for the library: langchain-mcp-adapters â€” covers MultiServerMCPClient, methods (session, get_tools, get_resources), and the load_mcp_tools / load_mcp_resources functions.\\n- GitHub repository (for reference implementations and examples): langchain-ai/langchain-mcp-adapters\\n\\nIf youâ€™d like, I can pull specific code examples or show a minimal working snippet tailored to your MCP server setup (e.g., a Python snippet that connects to two MCP servers, loads tools, and creates a simple agent). Just tell me what MCP servers youâ€™re planning to connect to (or share a rough config), and Iâ€™ll tailor the example.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2668, 'prompt_tokens': 2677, 'total_tokens': 5345, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1984, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Crfbm87iILlVnos9k9KVoFomZWmQm', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b63e2-b836-75a1-b55a-534d8d7c7cc2-0', usage_metadata={'input_tokens': 2677, 'output_tokens': 2668, 'total_tokens': 5345, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1984}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e117b",
   "metadata": {},
   "source": [
    "# Online MCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f328005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5974fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4220e44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='4751187a-3a2d-430a-867d-0b1513bb4c18'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 561, 'prompt_tokens': 296, 'total_tokens': 857, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 448, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CrfcTf12mpDJZRynUZpItkb59eBsf', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b63e3-622d-7283-91a6-34b34be087c1-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_ytqzHb0QqF2Bf9oyAzPe2obJ', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {'timezone': 'Europe/London'}, 'id': 'call_w53STFQ0c8bVsp5wAY0vHatw', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {'timezone': 'Asia/Tokyo'}, 'id': 'call_kquJkqw48kUyZrJLL4QXhS5W', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {'timezone': 'Australia/Sydney'}, 'id': 'call_gZRPmlJoLZl3QVWZlbfYtULN', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {'timezone': 'Etc/UTC'}, 'id': 'call_xMGsSU5ke5SCMus1kjAJV5hU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 296, 'output_tokens': 561, 'total_tokens': 857, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 448}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2025-12-28T02:36:55-05:00\",\\n  \"day_of_week\": \"Sunday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_4511bf34-aef6-4a6c-a609-5b9f07e638b1'}], name='get_current_time', id='993b9c29-6226-4af7-b496-b47529efbf2a', tool_call_id='call_ytqzHb0QqF2Bf9oyAzPe2obJ'),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"Europe/London\",\\n  \"datetime\": \"2025-12-28T07:36:55+00:00\",\\n  \"day_of_week\": \"Sunday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_6b1441fd-f19b-4877-803a-d032c6fd86b1'}], name='get_current_time', id='ab9721d0-8377-4a44-b9d9-ca1171387b65', tool_call_id='call_w53STFQ0c8bVsp5wAY0vHatw'),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"Asia/Tokyo\",\\n  \"datetime\": \"2025-12-28T16:36:55+09:00\",\\n  \"day_of_week\": \"Sunday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_302fb360-f274-4af2-8d25-3058352b2ad8'}], name='get_current_time', id='0f24e433-27f7-48d6-b142-be8f31ce3700', tool_call_id='call_kquJkqw48kUyZrJLL4QXhS5W'),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"Australia/Sydney\",\\n  \"datetime\": \"2025-12-28T18:36:54+11:00\",\\n  \"day_of_week\": \"Sunday\",\\n  \"is_dst\": true\\n}', 'id': 'lc_00c66278-694e-46e1-b751-291b759caf49'}], name='get_current_time', id='0e8dd851-5380-403e-bd65-7bbf6f910e98', tool_call_id='call_gZRPmlJoLZl3QVWZlbfYtULN'),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"Etc/UTC\",\\n  \"datetime\": \"2025-12-28T07:36:55+00:00\",\\n  \"day_of_week\": \"Sunday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_1e57a36a-89c2-4f35-886d-ef39d5f41348'}], name='get_current_time', id='11c299de-54c3-4bf5-8030-f25261f9334f', tool_call_id='call_xMGsSU5ke5SCMus1kjAJV5hU'),\n",
      "              AIMessage(content='Here are the current times in a few time zones (as of now):\\n\\n- America/New_York (Eastern Time): Sun 2025-12-28 02:36:55 EST\\n- Europe/London (GMT): Sun 2025-12-28 07:36:55 GMT\\n- Asia/Tokyo (JST): Sun 2025-12-28 16:36:55 JST\\n- Australia/Sydney (AEDT): Sun 2025-12-28 18:36:54 AEDT\\n- Etc/UTC (UTC): Sun 2025-12-28 07:36:55 UTC\\n\\nWant me to convert a time to another timezone or give you the time for a specific location?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1188, 'prompt_tokens': 635, 'total_tokens': 1823, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1024, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Crfcezcg9zrQX06AHUCd6dCiUamVw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b63e3-8d64-7382-8bfe-c128826ef342-0', usage_metadata={'input_tokens': 635, 'output_tokens': 1188, 'total_tokens': 1823, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1024}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
